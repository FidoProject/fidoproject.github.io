<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
<title>Fido: rl::QLearn Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="17830624.png"/></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">Fido
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Macros</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacerl.html">rl</a></li><li class="navelem"><a class="el" href="classrl_1_1_q_learn.html">QLearn</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classrl_1_1_q_learn-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">rl::QLearn Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>A <a class="el" href="classrl_1_1_learner.html" title="A reinforcement learning system. ">Learner</a> that follows the Q-Learning algorithm.  
 <a href="classrl_1_1_q_learn.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="_q_learn_8h_source.html">QLearn.h</a>&gt;</code></p>

<p>Inherits <a class="el" href="classrl_1_1_learner.html">rl::Learner</a>.</p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:ac83a21447898d1ad219a4f755e24520f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrl_1_1_q_learn.html#ac83a21447898d1ad219a4f755e24520f">QLearn</a> (<a class="el" href="classnet_1_1_neural_net.html">net::NeuralNet</a> *modelNetwork, <a class="el" href="classnet_1_1_backpropagation.html">net::Backpropagation</a> backprop_, double learningRate_, double devaluationFactor_, std::vector&lt; <a class="el" href="namespacerl.html#a3da872aded4c87056d470bd2916618a1">Action</a> &gt; possibleActions_)</td></tr>
<tr class="memdesc:ac83a21447898d1ad219a4f755e24520f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initializes a <a class="el" href="classrl_1_1_q_learn.html" title="A Learner that follows the Q-Learning algorithm. ">QLearn</a> object with a model network and the values of learning rate and devaluationFactor.  <a href="#ac83a21447898d1ad219a4f755e24520f">More...</a><br/></td></tr>
<tr class="separator:ac83a21447898d1ad219a4f755e24520f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae82bcbe67c7055209a88b6f911af1f47"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrl_1_1_q_learn.html#ae82bcbe67c7055209a88b6f911af1f47">QLearn</a> (std::vector&lt; <a class="el" href="structrl_1_1_model.html">Model</a> &gt; models_, <a class="el" href="classnet_1_1_backpropagation.html">net::Backpropagation</a> backprop_, double learningRate_, double devaluationFactor_)</td></tr>
<tr class="memdesc:ae82bcbe67c7055209a88b6f911af1f47"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initializes a <a class="el" href="classrl_1_1_q_learn.html" title="A Learner that follows the Q-Learning algorithm. ">QLearn</a> object with an vector of networks and the values of learning rate and devaluationFactor.  <a href="#ae82bcbe67c7055209a88b6f911af1f47">More...</a><br/></td></tr>
<tr class="separator:ae82bcbe67c7055209a88b6f911af1f47"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aecf618f5980d399fd60e935658035c81"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrl_1_1_q_learn.html#aecf618f5980d399fd60e935658035c81">QLearn</a> ()</td></tr>
<tr class="memdesc:aecf618f5980d399fd60e935658035c81"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initializes an empty, non-valid Q-learning object.  <a href="#aecf618f5980d399fd60e935658035c81">More...</a><br/></td></tr>
<tr class="separator:aecf618f5980d399fd60e935658035c81"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a209db7840a51554c937dbce73d4b79d9"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacerl.html#a3da872aded4c87056d470bd2916618a1">Action</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrl_1_1_q_learn.html#a209db7840a51554c937dbce73d4b79d9">chooseBestAction</a> (<a class="el" href="namespacerl.html#ab1b0f25d5634995bd741401cdf7677d3">State</a> currentState)</td></tr>
<tr class="memdesc:a209db7840a51554c937dbce73d4b79d9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the action that the network deems most benificial for the current state.  <a href="#a209db7840a51554c937dbce73d4b79d9">More...</a><br/></td></tr>
<tr class="separator:a209db7840a51554c937dbce73d4b79d9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7507815f38b376e53fa28802b0bfe1dd"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacerl.html#a3da872aded4c87056d470bd2916618a1">Action</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrl_1_1_q_learn.html#a7507815f38b376e53fa28802b0bfe1dd">chooseBoltzmanAction</a> (<a class="el" href="namespacerl.html#ab1b0f25d5634995bd741401cdf7677d3">State</a> currentState, double explorationConstant)</td></tr>
<tr class="memdesc:a7507815f38b376e53fa28802b0bfe1dd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets an action using the Boltzman softmax probability distribution.  <a href="#a7507815f38b376e53fa28802b0bfe1dd">More...</a><br/></td></tr>
<tr class="separator:a7507815f38b376e53fa28802b0bfe1dd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad0617a36dab296a5d1d88fe1a4e27a3f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrl_1_1_q_learn.html#ad0617a36dab296a5d1d88fe1a4e27a3f">applyReinforcementToLastAction</a> (double reward, <a class="el" href="namespacerl.html#ab1b0f25d5634995bd741401cdf7677d3">State</a> newState)</td></tr>
<tr class="memdesc:ad0617a36dab296a5d1d88fe1a4e27a3f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Updates expected reward values.  <a href="#ad0617a36dab296a5d1d88fe1a4e27a3f">More...</a><br/></td></tr>
<tr class="separator:ad0617a36dab296a5d1d88fe1a4e27a3f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad66a87f3ec9c7466fc310520bcd9edfb"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrl_1_1_q_learn.html#ad66a87f3ec9c7466fc310520bcd9edfb">reset</a> ()</td></tr>
<tr class="memdesc:ad66a87f3ec9c7466fc310520bcd9edfb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reverts the system to a newely initialized state.  <a href="#ad66a87f3ec9c7466fc310520bcd9edfb">More...</a><br/></td></tr>
<tr class="separator:ad66a87f3ec9c7466fc310520bcd9edfb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae680c12076a0451bb021b101bc196a6f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrl_1_1_q_learn.html#ae680c12076a0451bb021b101bc196a6f">store</a> (std::ofstream *output)</td></tr>
<tr class="memdesc:ae680c12076a0451bb021b101bc196a6f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Stores this model in a stream.  <a href="#ae680c12076a0451bb021b101bc196a6f">More...</a><br/></td></tr>
<tr class="separator:ae680c12076a0451bb021b101bc196a6f"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>A <a class="el" href="classrl_1_1_learner.html" title="A reinforcement learning system. ">Learner</a> that follows the Q-Learning algorithm. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="ac83a21447898d1ad219a4f755e24520f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">QLearn::QLearn </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classnet_1_1_neural_net.html">net::NeuralNet</a> *&#160;</td>
          <td class="paramname"><em>modelNetwork</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classnet_1_1_backpropagation.html">net::Backpropagation</a>&#160;</td>
          <td class="paramname"><em>backprop_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>devaluationFactor_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; <a class="el" href="namespacerl.html#a3da872aded4c87056d470bd2916618a1">Action</a> &gt;&#160;</td>
          <td class="paramname"><em>possibleActions_</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Initializes a <a class="el" href="classrl_1_1_q_learn.html" title="A Learner that follows the Q-Learning algorithm. ">QLearn</a> object with a model network and the values of learning rate and devaluationFactor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">modelNetwork</td><td>a neural network that is used as a model architecture for the networks that will rate the reward of each action. </td></tr>
    <tr><td class="paramname">backprop_</td><td>the model that will train the neural networks that will rate the reward of each action </td></tr>
    <tr><td class="paramname">learningRate_</td><td>a constant between 0 and 1 that dictates how fast the robot learns from reinforcement. </td></tr>
    <tr><td class="paramname">devaluationFactor_</td><td>a constant between 0 and 1 that weighs future reward vs immediate reward. A value of 0 will make the network only value immediate reward, while a value of 1 will make it consider future reward with the same weight as immediate reward. </td></tr>
    <tr><td class="paramname">possibleActions_</td><td>all of the actions that this object could possibly choose </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ae82bcbe67c7055209a88b6f911af1f47"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">QLearn::QLearn </td>
          <td>(</td>
          <td class="paramtype">std::vector&lt; <a class="el" href="structrl_1_1_model.html">Model</a> &gt;&#160;</td>
          <td class="paramname"><em>models_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classnet_1_1_backpropagation.html">net::Backpropagation</a>&#160;</td>
          <td class="paramname"><em>backprop_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>devaluationFactor_</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Initializes a <a class="el" href="classrl_1_1_q_learn.html" title="A Learner that follows the Q-Learning algorithm. ">QLearn</a> object with an vector of networks and the values of learning rate and devaluationFactor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">models_</td><td>he vector of networks contains the networks that will rate the reward of each action </td></tr>
    <tr><td class="paramname">backprop_</td><td>the model that will train the neural networks that will rate the reward of each action </td></tr>
    <tr><td class="paramname">learningRate_</td><td>a constant between 0 and 1 that dictates how fast the robot learns from reinforcement. </td></tr>
    <tr><td class="paramname">devaluationFactor_</td><td>a constant between 0 and 1 that weighs future reward vs immediate reward. A value of 0 will make the network only value immediate reward, while a value of 1 will make it consider future reward with the same weight as immediate reward. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="aecf618f5980d399fd60e935658035c81"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">QLearn::QLearn </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Initializes an empty, non-valid Q-learning object. </p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a class="anchor" id="ad0617a36dab296a5d1d88fe1a4e27a3f"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void QLearn::applyReinforcementToLastAction </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>reward</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacerl.html#ab1b0f25d5634995bd741401cdf7677d3">State</a>&#160;</td>
          <td class="paramname"><em>newState</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Updates expected reward values. </p>
<p>Given the immediate reward from the last action taken and the new state, this function updates the correct value for the longterm reward of the lastAction and trains the network in charge of the lastAction to output the corect reward value. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">reward</td><td>the reward value from the last action </td></tr>
    <tr><td class="paramname">newState</td><td>the new state (aka. inputs) to the system </td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classrl_1_1_learner.html#ac69ffdd45bfe4ceee924d490c6a7693f">rl::Learner</a>.</p>

</div>
</div>
<a class="anchor" id="a209db7840a51554c937dbce73d4b79d9"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacerl.html#a3da872aded4c87056d470bd2916618a1">Action</a> QLearn::chooseBestAction </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacerl.html#ab1b0f25d5634995bd741401cdf7677d3">State</a>&#160;</td>
          <td class="paramname"><em>currentState</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets the action that the network deems most benificial for the current state. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">currentState</td><td>a vector of doubles representing the "inputs" or sensor values of the system </td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classrl_1_1_learner.html#a0c0161ee9581f7ab456f13bbd69baec4">rl::Learner</a>.</p>

</div>
</div>
<a class="anchor" id="a7507815f38b376e53fa28802b0bfe1dd"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacerl.html#a3da872aded4c87056d470bd2916618a1">Action</a> QLearn::chooseBoltzmanAction </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacerl.html#ab1b0f25d5634995bd741401cdf7677d3">State</a>&#160;</td>
          <td class="paramname"><em>currentState</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>explorationConstant</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets an action using the Boltzman softmax probability distribution. </p>
<p>Non-random search heuristic used so that the neural network explores actions despite their reward value. The lower the exploration constanstant, the more likely it is to pick the best action for the current state. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">currentState</td><td>a vector of doubles representing the "inputs" or sensor values of the system </td></tr>
    <tr><td class="paramname">explorationConstant</td><td>a positive floating point number representing the exploration level of the system. Common values range from 0.01 to 1. The higher this number is, the more likely it is that the system will pick worse actions. </td></tr>
  </table>
  </dd>
</dl>
<p>Incase a floating point error resulted in no action </p>

<p>Implements <a class="el" href="classrl_1_1_learner.html#a626cd6c6a05357ccb3d392f34688629c">rl::Learner</a>.</p>

</div>
</div>
<a class="anchor" id="ad66a87f3ec9c7466fc310520bcd9edfb"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void QLearn::reset </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Reverts the system to a newely initialized state. </p>
<p>Reset's the system's model and wipes the system's memory of past actions, states, and rewards. </p>

<p>Implements <a class="el" href="classrl_1_1_learner.html#ad9b77c365288be0531fc7849fb3567a0">rl::Learner</a>.</p>

</div>
</div>
<a class="anchor" id="ae680c12076a0451bb021b101bc196a6f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void rl::QLearn::store </td>
          <td>(</td>
          <td class="paramtype">std::ofstream *&#160;</td>
          <td class="paramname"><em>output</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Stores this model in a stream. </p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>/home/truell20/Documents/Fido/include/<a class="el" href="_q_learn_8h_source.html">QLearn.h</a></li>
<li>/home/truell20/Documents/Fido/src/<a class="el" href="_q_learn_8cpp.html">QLearn.cpp</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Sat Apr 23 2016 15:38:44 for Fido by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.6
</small></address>
</body>
</html>
